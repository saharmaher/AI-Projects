{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f858631f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.isri import ISRIStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "efb81ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos=pd.read_csv('train_Arabic_tweets_positive_20190413.tsv',delimiter='\\t',names=['label','tweet'])\n",
    "train_neg=pd.read_csv('train_Arabic_tweets_negative_20190413.tsv',delimiter='\\t',names=['label','tweet'])\n",
    "\n",
    "test_pos=pd.read_csv('test_Arabic_tweets_positive_20190413.tsv',delimiter='\\t',names=['label','tweet'])\n",
    "test_neg=pd.read_csv('test_Arabic_tweets_negative_20190413.tsv',delimiter='\\t',names=['label','tweet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b22bc7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281298c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd011119",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('arabic')\n",
    "st = ISRIStemmer()\n",
    "stop=[]\n",
    "for w in stopwords_list:\n",
    "    rootWord=st.stem(w)\n",
    "    stop.append(rootWord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31bb330f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.concat([train_neg,train_pos],axis=0,ignore_index=True)\n",
    "\n",
    "test=pd.concat([test_neg,test_pos],axis=0)\n",
    "s=train['tweet'].str.len()\n",
    "train_neg=[]\n",
    "train_pos=[]\n",
    "test_pos=[]\n",
    "test_neg=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "885134c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    stemmer = nltk.ISRIStemmer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    #remove arabic stopwords\n",
    "    word_list = [ w for w in word_list if not w in stopwords_list ]\n",
    "    #remove digits\n",
    "    word_list = [ w for w in word_list ]\n",
    "    #stemming\n",
    "    word_list = [stemmer.stem(w) for w in  word_list]\n",
    "    return ' '.join(word_list) \n",
    "\n",
    "\n",
    "def clean_text(text):  \n",
    "\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\n",
    "              \"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\n",
    "               \"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ', ' ! ']\n",
    "    #remove tashkeel\n",
    "    tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(tashkeel,\"\", text)\n",
    "  \n",
    "    longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(longation, subst, text)\n",
    "    \n",
    "    text = re.sub(r\"[^\\w\\s]\", '', text)\n",
    "    #remove english words\n",
    "    text = re.sub(r\"[a-zA-Z]\", '', text)\n",
    "    #remove spaces\n",
    "    text = re.sub(r\"\\d+\", ' ', text)\n",
    "    text = re.sub(r\"\\n+\", ' ', text)\n",
    "    text = re.sub(r\"\\t+\", ' ', text)\n",
    "    text = re.sub(r\"\\r+\", ' ', text)\n",
    "    text = re.sub(r\"\\s+\", ' ', text)\n",
    "    #remove repetetions\n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    return process_text(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22274970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "326cf7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['tweet2']=train.tweet.apply(clean_text)\n",
    "test['tweet2']=test.tweet.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69facd3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tweet2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neg</td>\n",
       "      <td>اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...</td>\n",
       "      <td>عرف ان بتس كنو شوي شوي جيبو رسي اليوم زيد</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neg</td>\n",
       "      <td>توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...</td>\n",
       "      <td>وقع اذا جات دار بشف كمل حين احس احد نقص</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neg</td>\n",
       "      <td>#الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...</td>\n",
       "      <td>اهل هلل كتب وقع نتج لقء هلل اهل تاق تحد سرع رو...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>neg</td>\n",
       "      <td>نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...</td>\n",
       "      <td>نعم ضاد حيي تضع قطرهمضاد نسل علي كتر فجر تمو ا...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neg</td>\n",
       "      <td>الدودو جايه تكمل علي 💔</td>\n",
       "      <td>دودو جيه كمل علي</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45270</th>\n",
       "      <td>pos</td>\n",
       "      <td>السحب الليلة على الايفون .. رتويت للمرفقة وطبق...</td>\n",
       "      <td>سحب ليل علي ايف رتي رفق طبق شرط</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45271</th>\n",
       "      <td>pos</td>\n",
       "      <td>😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂</td>\n",
       "      <td>لبس حمر ليه يست انت ايه نسب</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45272</th>\n",
       "      <td>pos</td>\n",
       "      <td>كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...</td>\n",
       "      <td>كلم جمل تستاهلمن احب الل حبت قلب بشر</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45273</th>\n",
       "      <td>pos</td>\n",
       "      <td>- ألطف صورة ممكن تعبر عن رمضان 💙</td>\n",
       "      <td>لطف صور مكن عبر رمض</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45274</th>\n",
       "      <td>pos</td>\n",
       "      <td>🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...</td>\n",
       "      <td>قال امم ابن قيم رحم الل علي فان ير نعم الل الا...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45275 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      label                                              tweet  \\\n",
       "0       neg  اعترف ان بتس كانو شوي شوي يجيبو راسي لكن اليوم...   \n",
       "1       neg  توقعت اذا جات داريا بشوفهم كاملين بس لي للحين ...   \n",
       "2       neg  #الاهلي_الهلال اكتب توقعك لنتيجة لقاء الهلال و...   \n",
       "3       neg  نعمة المضادات الحيوية . تضع قطرة💧مضاد بنسلين ع...   \n",
       "4       neg                             الدودو جايه تكمل علي 💔   \n",
       "...     ...                                                ...   \n",
       "45270   pos  السحب الليلة على الايفون .. رتويت للمرفقة وطبق...   \n",
       "45271   pos         😂 لابسة احمر ليه يا ست انتي ايه المناسبة 😂   \n",
       "45272   pos  كلاام جمييل تستاهل(من احبه الله جعل محبته ف قل...   \n",
       "45273   pos                   - ألطف صورة ممكن تعبر عن رمضان 💙   \n",
       "45274   pos  🌸 قال #الإمام_ابن_القيم -رحمه الله تعالى- : - ...   \n",
       "\n",
       "                                                  tweet2  \n",
       "0              عرف ان بتس كنو شوي شوي جيبو رسي اليوم زيد  \n",
       "1                وقع اذا جات دار بشف كمل حين احس احد نقص  \n",
       "2      اهل هلل كتب وقع نتج لقء هلل اهل تاق تحد سرع رو...  \n",
       "3      نعم ضاد حيي تضع قطرهمضاد نسل علي كتر فجر تمو ا...  \n",
       "4                                       دودو جيه كمل علي  \n",
       "...                                                  ...  \n",
       "45270                    سحب ليل علي ايف رتي رفق طبق شرط  \n",
       "45271                        لبس حمر ليه يست انت ايه نسب  \n",
       "45272               كلم جمل تستاهلمن احب الل حبت قلب بشر  \n",
       "45273                                لطف صور مكن عبر رمض  \n",
       "45274  قال امم ابن قيم رحم الل علي فان ير نعم الل الا...  \n",
       "\n",
       "[45275 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d63d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=train.tweet\n",
    "X_test=test.tweet\n",
    "y_train=train['label']\n",
    "y_test=test['label']\n",
    "train=[]\n",
    "\n",
    "X_train.drop_duplicates()\n",
    "\n",
    "enc=LabelEncoder()\n",
    "y_train=enc.fit_transform(y_train)\n",
    "y_test=enc.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a490a331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 78651\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "print(\"vocab size:\",len(tokenizer.word_index))\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=300)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be043e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 300, 100)          2000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 300, 200)         160800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 200)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               25728     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,196,897\n",
      "Trainable params: 2,196,897\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#create LSTM model with keras\n",
    "embedding_dim = 100\n",
    "dropout = 0.5\n",
    "opt = 'adam'\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(input_dim=20000, \n",
    "                           output_dim=100, \n",
    "                           input_length=300))\n",
    "model.add(layers.Bidirectional(layers.LSTM(100, dropout=0.5, \n",
    "                                           recurrent_dropout=0.5, \n",
    "                                           return_sequences=True)))\n",
    "model.add(layers.GlobalMaxPool1D())\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(32, activation='relu'))\n",
    "model.add(layers.Dropout(dropout))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=opt, \n",
    "              loss='binary_crossentropy', \n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1b712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "177/177 [==============================] - 8199s 46s/step - loss: 0.5197 - accuracy: 0.6877\n",
      "Epoch 2/4\n",
      " 29/177 [===>..........................] - ETA: 1:03:59 - loss: 0.1701 - accuracy: 0.9440"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=4,\n",
    "                    verbose=True,\n",
    "                    validation_freq=0.2,\n",
    "                    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0231fdbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf065b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
